{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":11004604,"sourceType":"datasetVersion","datasetId":6850670}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport cv2\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential, Model\nfrom tensorflow.keras.layers import Dense, Dropout, LSTM, Conv2D, MaxPooling2D, Flatten, TimeDistributed, GlobalAveragePooling2D, Input\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\nfrom tensorflow.keras.utils import to_categorical, Sequence\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.applications import MobileNetV2\nimport glob\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nimport gc","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T15:55:29.156262Z","iopub.execute_input":"2025-03-13T15:55:29.156560Z","iopub.status.idle":"2025-03-13T15:55:32.567065Z","shell.execute_reply.started":"2025-03-13T15:55:29.156537Z","shell.execute_reply":"2025-03-13T15:55:32.566130Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"# Set memory growth for GPU\nphysical_devices = tf.config.list_physical_devices('GPU')\nif len(physical_devices) > 0:\n    tf.config.experimental.set_memory_growth(physical_devices[0], True)\n\n# Configuration parameters\nCONFIG = {\n    'seed': 42,\n    'target_size': (224, 224),  # Resizing frames to save memory\n    'batch_size': 4,  # Small batch size to avoid memory issues\n    'epochs': 10,\n    'frames_per_video': 16,  # Taking subset of frames to save memory\n    'learning_rate': 1e-4,\n    'num_classes': 2,  # Shoplifter vs Non-shoplifter\n}\n\n# Set random seeds for reproducibility\ntf.random.set_seed(CONFIG['seed'])\nnp.random.seed(CONFIG['seed'])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T15:55:32.568115Z","iopub.execute_input":"2025-03-13T15:55:32.568616Z","iopub.status.idle":"2025-03-13T15:55:32.678254Z","shell.execute_reply.started":"2025-03-13T15:55:32.568591Z","shell.execute_reply":"2025-03-13T15:55:32.677042Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"# Create a DataFrame with video paths and labels\ndef create_dataframe(shoplifters_dir, non_shoplifters_dir):\n    shoplifters_paths = glob.glob(os.path.join(shoplifters_dir, '*.mp4'))\n    non_shoplifters_paths = glob.glob(os.path.join(non_shoplifters_dir, '*.mp4'))\n    \n    shoplifters_df = pd.DataFrame({\n        'path': shoplifters_paths,\n        'label': 1  # 1 for shoplifter\n    })\n    \n    non_shoplifters_df = pd.DataFrame({\n        'path': non_shoplifters_paths,\n        'label': 0  # 0 for non-shoplifter\n    })\n    \n    df = pd.concat([shoplifters_df, non_shoplifters_df], ignore_index=True)\n    return df\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T15:55:35.422131Z","iopub.execute_input":"2025-03-13T15:55:35.422500Z","iopub.status.idle":"2025-03-13T15:55:35.427606Z","shell.execute_reply.started":"2025-03-13T15:55:35.422472Z","shell.execute_reply":"2025-03-13T15:55:35.426628Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"# Exploratory Data Analysis\ndef perform_eda(df, shoplifters_dir, non_shoplifters_dir):\n    print(\"Data Summary:\")\n    print(f\"Total videos: {len(df)}\")\n    print(f\"Shoplifter videos: {len(df[df['label'] == 1])}\")\n    print(f\"Non-shoplifter videos: {len(df[df['label'] == 0])}\")\n    \n    # Sample a few videos to analyze\n    sample_videos = df.sample(min(10, len(df)), random_state=CONFIG['seed'])\n    \n    video_stats = []\n    for _, row in sample_videos.iterrows():\n        video_path = row['path']\n        cap = cv2.VideoCapture(video_path)\n        \n        # Get video properties\n        frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n        fps = cap.get(cv2.CAP_PROP_FPS)\n        width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n        height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n        duration = frame_count / fps\n        \n        video_stats.append({\n            'path': video_path,\n            'label': row['label'],\n            'frame_count': frame_count,\n            'fps': fps,\n            'width': width,\n            'height': height,\n            'duration': duration\n        })\n        \n        cap.release()\n    \n    stats_df = pd.DataFrame(video_stats)\n    print(\"\\nVideo Statistics:\")\n    print(stats_df.describe())\n    \n    # Plot distribution of frame counts\n    plt.figure(figsize=(10, 6))\n    sns.histplot(data=stats_df, x='frame_count', hue='label', \n                 element='step', common_norm=False, bins=20)\n    plt.title('Distribution of Frame Counts')\n    plt.xlabel('Number of Frames')\n    plt.ylabel('Count')\n    plt.legend(['Non-Shoplifter', 'Shoplifter'])\n    plt.savefig('frame_count_distribution.png')\n    plt.close()\n    \n    # Plot distribution of video durations\n    plt.figure(figsize=(10, 6))\n    sns.histplot(data=stats_df, x='duration', hue='label', \n                 element='step', common_norm=False, bins=20)\n    plt.title('Distribution of Video Durations')\n    plt.xlabel('Duration (seconds)')\n    plt.ylabel('Count')\n    plt.legend(['Non-Shoplifter', 'Shoplifter'])\n    plt.savefig('duration_distribution.png')\n    plt.close()\n    \n    return stats_df\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T15:55:36.884766Z","iopub.execute_input":"2025-03-13T15:55:36.885068Z","iopub.status.idle":"2025-03-13T15:55:36.893487Z","shell.execute_reply.started":"2025-03-13T15:55:36.885044Z","shell.execute_reply":"2025-03-13T15:55:36.892483Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"# Video Data Generator\nclass VideoDataGenerator(Sequence):\n    def __init__(self, dataframe, batch_size, frames_per_video, target_size, num_classes, shuffle=True):\n        self.dataframe = dataframe\n        self.batch_size = batch_size\n        self.frames_per_video = frames_per_video\n        self.target_size = target_size\n        self.num_classes = num_classes\n        self.shuffle = shuffle\n        self.indexes = np.arange(len(dataframe))\n        if self.shuffle:\n            np.random.shuffle(self.indexes)\n            \n        # Image data augmentation\n        self.img_gen = ImageDataGenerator(\n            rescale=1./255,\n            rotation_range=10,\n            width_shift_range=0.1,\n            height_shift_range=0.1,\n            shear_range=0.1,\n            zoom_range=0.1,\n            horizontal_flip=True,\n            fill_mode='nearest'\n        )\n        \n        # Handle empty batches issue by preprocessing to identify valid videos\n        self.valid_indices = []\n        for i in range(len(self.dataframe)):\n            video_path = self.dataframe.iloc[i]['path']\n            if self._check_video_valid(video_path):\n                self.valid_indices.append(i)\n                \n        self.indexes = np.array(self.valid_indices)\n        if self.shuffle:\n            np.random.shuffle(self.indexes)\n        \n    def _check_video_valid(self, video_path):\n        \"\"\"Check if video has enough frames to extract.\"\"\"\n        try:\n            cap = cv2.VideoCapture(video_path)\n            frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n            cap.release()\n            return frame_count >= self.frames_per_video\n        except:\n            return False\n    \n    def __len__(self):\n        \"\"\"Denotes the number of batches per epoch\"\"\"\n        return int(np.ceil(len(self.indexes) / self.batch_size))\n    \n    def __getitem__(self, idx):\n        \"\"\"Generate one batch of data\"\"\"\n        # Generate indexes of the batch\n        batch_indexes = self.indexes[idx * self.batch_size:(idx + 1) * self.batch_size]\n        \n        # Find list of IDs\n        batch_df = self.dataframe.iloc[batch_indexes]\n        \n        batch_videos = []\n        batch_labels = []\n        \n        for _, row in batch_df.iterrows():\n            video_path = row['path']\n            label = row['label']\n            \n            # Process video\n            frames = self._extract_frames(video_path)\n            if frames is not None and len(frames) == self.frames_per_video:\n                batch_videos.append(frames)\n                batch_labels.append(label)\n        \n        # Ensure we have at least one valid video in the batch\n        if len(batch_videos) == 0:\n            # If no valid videos in batch, use first valid video\n            first_valid_path = self.dataframe.iloc[self.indexes[0]]['path']\n            first_valid_label = self.dataframe.iloc[self.indexes[0]]['label']\n            frames = self._extract_frames(first_valid_path)\n            if frames is not None:\n                batch_videos.append(frames)\n                batch_labels.append(first_valid_label)\n        \n        # Convert to numpy arrays\n        batch_videos = np.array(batch_videos)\n        batch_labels = to_categorical(np.array(batch_labels), num_classes=self.num_classes)\n        \n        return batch_videos, batch_labels\n    \n    def _extract_frames(self, video_path):\n        try:\n            cap = cv2.VideoCapture(video_path)\n            frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n            \n            if frame_count <= 0:\n                cap.release()\n                return None\n            \n            # Calculate indices of frames to extract\n            indices = np.linspace(0, frame_count - 1, self.frames_per_video, dtype=int)\n            \n            frames = []\n            for i in indices:\n                cap.set(cv2.CAP_PROP_POS_FRAMES, i)\n                ret, frame = cap.read()\n                \n                if not ret:\n                    continue\n                \n                # Convert BGR to RGB\n                frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n                \n                # Resize frame\n                frame = cv2.resize(frame, self.target_size)\n                \n                # Apply image augmentation\n                frame = self.img_gen.random_transform(frame)\n                \n                # Normalize\n                frame = frame / 255.0\n                \n                frames.append(frame)\n            \n            cap.release()\n            \n            # If we couldn't extract enough frames, return None\n            if len(frames) < self.frames_per_video:\n                return None\n            \n            return np.array(frames)\n        except Exception as e:\n            print(f\"Error processing video {video_path}: {e}\")\n            return None\n    \n    def on_epoch_end(self):\n        \"\"\"Updates indexes after each epoch\"\"\"\n        if self.shuffle:\n            np.random.shuffle(self.indexes)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T15:55:38.293936Z","iopub.execute_input":"2025-03-13T15:55:38.294258Z","iopub.status.idle":"2025-03-13T15:55:38.307050Z","shell.execute_reply.started":"2025-03-13T15:55:38.294232Z","shell.execute_reply":"2025-03-13T15:55:38.306125Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"def create_model(input_shape, num_classes):\n    \"\"\"\n    Create an improved video classification model for shoplifting detection\n    with class imbalance handling\n    \n    Parameters:\n    - input_shape: Shape of input data (frames, height, width, channels)\n    - num_classes: Number of output classes\n    \n    Returns:\n    - Compiled Keras model\n    \"\"\"\n    # Import required layers\n    from tensorflow.keras.layers import Conv2D, BatchNormalization, MaxPooling2D, Flatten\n    from tensorflow.keras.layers import Dense, Dropout, LSTM, TimeDistributed, Input\n    from tensorflow.keras.layers import GlobalAveragePooling2D, Bidirectional\n    from tensorflow.keras.models import Model\n    from tensorflow.keras.optimizers import Adam\n    \n    # Input shape\n    input_layer = Input(shape=input_shape)\n    \n    # First Conv block - extract basic features\n    x = TimeDistributed(Conv2D(32, (3, 3), activation='relu', padding='same'))(input_layer)\n    x = TimeDistributed(BatchNormalization())(x)\n    x = TimeDistributed(MaxPooling2D((2, 2)))(x)\n    \n    # Second Conv block - more complex features\n    x = TimeDistributed(Conv2D(64, (3, 3), activation='relu', padding='same'))(x)\n    x = TimeDistributed(BatchNormalization())(x)\n    x = TimeDistributed(MaxPooling2D((2, 2)))(x)\n    \n    # Third Conv block - detect higher level patterns\n    x = TimeDistributed(Conv2D(128, (3, 3), activation='relu', padding='same'))(x)\n    x = TimeDistributed(Conv2D(128, (3, 3), activation='relu', padding='same'))(x)  # Added depth\n    x = TimeDistributed(BatchNormalization())(x)\n    x = TimeDistributed(MaxPooling2D((2, 2)))(x)\n    \n    # Fourth Conv block - more specific features\n    x = TimeDistributed(Conv2D(256, (3, 3), activation='relu', padding='same'))(x)\n    x = TimeDistributed(Conv2D(256, (3, 3), activation='relu', padding='same'))(x)  # Added depth\n    x = TimeDistributed(BatchNormalization())(x)\n    x = TimeDistributed(MaxPooling2D((2, 2)))(x)\n    \n    # Use GlobalAveragePooling2D instead of Flatten to reduce parameters\n    x = TimeDistributed(GlobalAveragePooling2D())(x)\n    \n    # Use Bidirectional LSTM for better temporal pattern detection\n    x = Bidirectional(LSTM(256, return_sequences=True))(x)\n    x = Dropout(0.4)(x)\n    x = Bidirectional(LSTM(128))(x)\n    x = Dropout(0.4)(x)\n    \n    # Dense layers for classification\n    x = Dense(128, activation='relu')(x)\n    x = BatchNormalization()(x)\n    x = Dropout(0.5)(x)\n    \n    # Output layer\n    output = Dense(num_classes, activation='softmax')(x)\n    \n    # Create model\n    model = Model(inputs=input_layer, outputs=output)\n    \n    # Compile model with class weighting considerations\n    model.compile(\n        optimizer=Adam(learning_rate=1e-4),\n        loss='categorical_crossentropy',\n        metrics=['accuracy', \n                tf.keras.metrics.Precision(name='precision'),\n                tf.keras.metrics.Recall(name='recall'),\n                tf.keras.metrics.AUC(name='auc')]\n    )\n    \n    return model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T15:55:41.060537Z","iopub.execute_input":"2025-03-13T15:55:41.060836Z","iopub.status.idle":"2025-03-13T15:55:41.070510Z","shell.execute_reply.started":"2025-03-13T15:55:41.060814Z","shell.execute_reply":"2025-03-13T15:55:41.069448Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"# Function to train the model\ndef train_model(train_gen, val_gen, model, epochs):\n    # Callbacks\n    checkpoint = ModelCheckpoint(\n        'best_model.keras',\n        monitor='val_accuracy',\n        save_best_only=True,\n        mode='max',\n        verbose=1\n    )\n    \n    early_stopping = EarlyStopping(\n        monitor='val_accuracy',\n        patience=5,\n        restore_best_weights=True,\n        verbose=1\n    )\n    \n    reduce_lr = ReduceLROnPlateau(\n        monitor='val_loss',\n        factor=0.2,\n        patience=3,\n        min_lr=1e-6,\n        verbose=1\n    )\n    \n    # Train the model\n    history = model.fit(\n        train_gen,\n        validation_data=val_gen,\n        epochs=epochs,\n        callbacks=[checkpoint, early_stopping, reduce_lr]\n    )\n    \n    return history, model\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T15:55:42.903565Z","iopub.execute_input":"2025-03-13T15:55:42.903868Z","iopub.status.idle":"2025-03-13T15:55:42.908796Z","shell.execute_reply.started":"2025-03-13T15:55:42.903846Z","shell.execute_reply":"2025-03-13T15:55:42.907774Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"# Function to evaluate the model\ndef evaluate_model(model, test_gen):\n    # Initialize arrays for predictions and ground truth\n    all_predictions = []\n    all_true_labels = []\n    \n    # Loop through the test generator\n    for i in range(len(test_gen)):\n        x, y = test_gen[i]\n        \n        # Get model predictions\n        pred = model.predict(x)\n        \n        # Store predictions and true labels\n        all_predictions.extend(np.argmax(pred, axis=1))\n        all_true_labels.extend(np.argmax(y, axis=1))\n    \n    # Calculate metrics\n    from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n    \n    # Accuracy\n    acc = accuracy_score(all_true_labels, all_predictions)\n    print(f\"\\nTest Accuracy: {acc:.4f}\")\n    \n    # Classification report\n    print(\"\\nClassification Report:\")\n    print(classification_report(all_true_labels, all_predictions, target_names=['Non-Shoplifter', 'Shoplifter']))\n    \n    # Confusion matrix\n    print(\"\\nConfusion Matrix:\")\n    cm = confusion_matrix(all_true_labels, all_predictions)\n    print(cm)\n    \n    # Plot confusion matrix\n    plt.figure(figsize=(8, 6))\n    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n                xticklabels=['Non-Shoplifter', 'Shoplifter'],\n                yticklabels=['Non-Shoplifter', 'Shoplifter'])\n    plt.xlabel('Predicted')\n    plt.ylabel('True')\n    plt.title('Confusion Matrix')\n    plt.savefig('confusion_matrix.png')\n    plt.close()\n    \n    return all_true_labels, all_predictions\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T15:55:44.361364Z","iopub.execute_input":"2025-03-13T15:55:44.361690Z","iopub.status.idle":"2025-03-13T15:55:44.368090Z","shell.execute_reply.started":"2025-03-13T15:55:44.361670Z","shell.execute_reply":"2025-03-13T15:55:44.367048Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"# Improved main function with class weight handling\ndef main():\n    # Set paths to your data directories\n    shoplifters_dir = '/kaggle/input/shoplifters/Shop DataSet/shop lifters'\n    non_shoplifters_dir = '/kaggle/input/shoplifters/Shop DataSet/non shop lifters'\n    \n    # Create dataframe\n    df = create_dataframe(shoplifters_dir, non_shoplifters_dir)\n    \n    # Perform EDA\n    stats_df = perform_eda(df, shoplifters_dir, non_shoplifters_dir)\n    \n    # Split the data\n    train_df, temp_df = train_test_split(df, test_size=0.4, random_state=CONFIG['seed'], stratify=df['label'])\n    val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=CONFIG['seed'], stratify=temp_df['label'])\n    \n    print(f\"Train set: {len(train_df)} videos\")\n    print(f\"- Shoplifters: {len(train_df[train_df['label'] == 1])}\")\n    print(f\"- Non-shoplifters: {len(train_df[train_df['label'] == 0])}\")\n    \n    print(f\"Validation set: {len(val_df)} videos\")\n    print(f\"- Shoplifters: {len(val_df[val_df['label'] == 1])}\")\n    print(f\"- Non-shoplifters: {len(val_df[val_df['label'] == 0])}\")\n    \n    print(f\"Test set: {len(test_df)} videos\")\n    print(f\"- Shoplifters: {len(test_df[test_df['label'] == 1])}\")\n    print(f\"- Non-shoplifters: {len(test_df[test_df['label'] == 0])}\")\n    \n    # Calculate class weights to handle imbalance\n    total = len(df)\n    n_shoplifters = len(df[df['label'] == 1])\n    n_non_shoplifters = len(df[df['label'] == 0])\n    \n    class_weight = {\n        0: total / (2.0 * n_non_shoplifters),  # Weight for non-shoplifters\n        1: total / (2.0 * n_shoplifters)       # Weight for shoplifters\n    }\n    print(f\"Using class weights: {class_weight}\")\n    \n    # Create data generators with balanced batch sampling\n    train_gen = VideoDataGenerator(\n        train_df,\n        batch_size=CONFIG['batch_size'],\n        frames_per_video=CONFIG['frames_per_video'],\n        target_size=CONFIG['target_size'],\n        num_classes=CONFIG['num_classes'],\n        shuffle=True\n    )\n    \n    val_gen = VideoDataGenerator(\n        val_df,\n        batch_size=CONFIG['batch_size'],\n        frames_per_video=CONFIG['frames_per_video'],\n        target_size=CONFIG['target_size'],\n        num_classes=CONFIG['num_classes'],\n        shuffle=False\n    )\n    \n    test_gen = VideoDataGenerator(\n        test_df,\n        batch_size=CONFIG['batch_size'],\n        frames_per_video=CONFIG['frames_per_video'],\n        target_size=CONFIG['target_size'],\n        num_classes=CONFIG['num_classes'],\n        shuffle=False\n    )\n    \n    # Create the model\n    input_shape = (CONFIG['frames_per_video'], CONFIG['target_size'][0], CONFIG['target_size'][1], 3)\n    model = create_model(input_shape, CONFIG['num_classes'])\n    \n    # Print model summary\n    model.summary()\n    \n    # Train the model with class weights\n    history = model.fit(\n        train_gen,\n        validation_data=val_gen,\n        epochs=CONFIG['epochs'],\n        class_weight=class_weight,  # Add class weights\n        callbacks=[\n            ModelCheckpoint('best_model.keras', monitor='val_auc', save_best_only=True, mode='max', verbose=1),\n            EarlyStopping(monitor='val_auc', patience=5, restore_best_weights=True, verbose=1),\n            ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, min_lr=1e-6, verbose=1)\n        ]\n    )\n    \n    # Plot training history with additional metrics\n    plt.figure(figsize=(16, 10))\n    \n    # Plot accuracy\n    plt.subplot(2, 2, 1)\n    plt.plot(history.history['accuracy'])\n    plt.plot(history.history['val_accuracy'])\n    plt.title('Model Accuracy')\n    plt.xlabel('Epoch')\n    plt.ylabel('Accuracy')\n    plt.legend(['Train', 'Validation'], loc='lower right')\n    \n    # Plot loss\n    plt.subplot(2, 2, 2)\n    plt.plot(history.history['loss'])\n    plt.plot(history.history['val_loss'])\n    plt.title('Model Loss')\n    plt.xlabel('Epoch')\n    plt.ylabel('Loss')\n    plt.legend(['Train', 'Validation'], loc='upper right')\n    \n    # Plot precision\n    plt.subplot(2, 2, 3)\n    plt.plot(history.history['precision'])\n    plt.plot(history.history['val_precision'])\n    plt.title('Model Precision')\n    plt.xlabel('Epoch')\n    plt.ylabel('Precision')\n    plt.legend(['Train', 'Validation'], loc='lower right')\n    \n    # Plot recall\n    plt.subplot(2, 2, 4)\n    plt.plot(history.history['recall'])\n    plt.plot(history.history['val_recall'])\n    plt.title('Model Recall')\n    plt.xlabel('Epoch')\n    plt.ylabel('Recall')\n    plt.legend(['Train', 'Validation'], loc='lower right')\n    \n    plt.tight_layout()\n    plt.savefig('training_history.png')\n    plt.close()\n    \n    # Evaluate the model\n    y_true, y_pred = evaluate_model(model, test_gen)\n    \n    # Clean up to save memory\n    del train_gen, val_gen, test_gen\n    gc.collect()\n    \n    print(\"Model training and evaluation completed!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T15:55:45.733877Z","iopub.execute_input":"2025-03-13T15:55:45.734187Z","iopub.status.idle":"2025-03-13T15:55:45.746356Z","shell.execute_reply.started":"2025-03-13T15:55:45.734163Z","shell.execute_reply":"2025-03-13T15:55:45.745440Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"if __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T15:55:48.602106Z","iopub.execute_input":"2025-03-13T15:55:48.602471Z","iopub.status.idle":"2025-03-13T17:18:33.832184Z","shell.execute_reply.started":"2025-03-13T15:55:48.602443Z","shell.execute_reply":"2025-03-13T17:18:33.831292Z"}},"outputs":[{"name":"stdout","text":"Data Summary:\nTotal videos: 855\nShoplifter videos: 324\nNon-shoplifter videos: 531\n\nVideo Statistics:\n           label  frame_count        fps  width  height   duration\ncount  10.000000    10.000000  10.000000   10.0    10.0  10.000000\nmean    0.500000   321.600000  24.908000  704.0   576.0  12.911585\nstd     0.527046    93.768272   0.120996    0.0     0.0   3.753895\nmin     0.000000   225.000000  24.750000  704.0   576.0   9.000000\n25%     0.000000   255.750000  24.770000  704.0   576.0  10.257576\n50%     0.500000   287.000000  25.000000  704.0   576.0  11.517519\n75%     1.000000   365.750000  25.000000  704.0   576.0  14.777778\nmax     1.000000   475.000000  25.000000  704.0   576.0  19.000000\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context('mode.use_inf_as_na', True):\n/usr/local/lib/python3.10/dist-packages/seaborn/_oldcore.py:1075: FutureWarning: When grouping with a length-1 list-like, you will need to pass a length-1 tuple to get_group in a future version of pandas. Pass `(name,)` instead of `name` to silence this warning.\n  data_subset = grouped_data.get_group(pd_key)\n/usr/local/lib/python3.10/dist-packages/seaborn/_oldcore.py:1075: FutureWarning: When grouping with a length-1 list-like, you will need to pass a length-1 tuple to get_group in a future version of pandas. Pass `(name,)` instead of `name` to silence this warning.\n  data_subset = grouped_data.get_group(pd_key)\n/usr/local/lib/python3.10/dist-packages/seaborn/_oldcore.py:1075: FutureWarning: When grouping with a length-1 list-like, you will need to pass a length-1 tuple to get_group in a future version of pandas. Pass `(name,)` instead of `name` to silence this warning.\n  data_subset = grouped_data.get_group(pd_key)\n/usr/local/lib/python3.10/dist-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context('mode.use_inf_as_na', True):\n/usr/local/lib/python3.10/dist-packages/seaborn/_oldcore.py:1075: FutureWarning: When grouping with a length-1 list-like, you will need to pass a length-1 tuple to get_group in a future version of pandas. Pass `(name,)` instead of `name` to silence this warning.\n  data_subset = grouped_data.get_group(pd_key)\n/usr/local/lib/python3.10/dist-packages/seaborn/_oldcore.py:1075: FutureWarning: When grouping with a length-1 list-like, you will need to pass a length-1 tuple to get_group in a future version of pandas. Pass `(name,)` instead of `name` to silence this warning.\n  data_subset = grouped_data.get_group(pd_key)\n","output_type":"stream"},{"name":"stdout","text":"Train set: 513 videos\n- Shoplifters: 194\n- Non-shoplifters: 319\nValidation set: 171 videos\n- Shoplifters: 65\n- Non-shoplifters: 106\nTest set: 171 videos\n- Shoplifters: 65\n- Non-shoplifters: 106\nUsing class weights: {0: 0.8050847457627118, 1: 1.3194444444444444}\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"functional\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n│ input_layer (\u001b[38;5;33mInputLayer\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m3\u001b[0m)     │               \u001b[38;5;34m0\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ time_distributed (\u001b[38;5;33mTimeDistributed\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m32\u001b[0m)    │             \u001b[38;5;34m896\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ time_distributed_1 (\u001b[38;5;33mTimeDistributed\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m32\u001b[0m)    │             \u001b[38;5;34m128\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ time_distributed_2 (\u001b[38;5;33mTimeDistributed\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m112\u001b[0m, \u001b[38;5;34m112\u001b[0m, \u001b[38;5;34m32\u001b[0m)    │               \u001b[38;5;34m0\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ time_distributed_3 (\u001b[38;5;33mTimeDistributed\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m112\u001b[0m, \u001b[38;5;34m112\u001b[0m, \u001b[38;5;34m64\u001b[0m)    │          \u001b[38;5;34m18,496\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ time_distributed_4 (\u001b[38;5;33mTimeDistributed\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m112\u001b[0m, \u001b[38;5;34m112\u001b[0m, \u001b[38;5;34m64\u001b[0m)    │             \u001b[38;5;34m256\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ time_distributed_5 (\u001b[38;5;33mTimeDistributed\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m64\u001b[0m)      │               \u001b[38;5;34m0\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ time_distributed_6 (\u001b[38;5;33mTimeDistributed\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m128\u001b[0m)     │          \u001b[38;5;34m73,856\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ time_distributed_7 (\u001b[38;5;33mTimeDistributed\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m128\u001b[0m)     │         \u001b[38;5;34m147,584\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ time_distributed_8 (\u001b[38;5;33mTimeDistributed\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m128\u001b[0m)     │             \u001b[38;5;34m512\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ time_distributed_9 (\u001b[38;5;33mTimeDistributed\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m128\u001b[0m)     │               \u001b[38;5;34m0\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ time_distributed_10                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m256\u001b[0m)     │         \u001b[38;5;34m295,168\u001b[0m │\n│ (\u001b[38;5;33mTimeDistributed\u001b[0m)                    │                             │                 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ time_distributed_11                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m256\u001b[0m)     │         \u001b[38;5;34m590,080\u001b[0m │\n│ (\u001b[38;5;33mTimeDistributed\u001b[0m)                    │                             │                 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ time_distributed_12                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m256\u001b[0m)     │           \u001b[38;5;34m1,024\u001b[0m │\n│ (\u001b[38;5;33mTimeDistributed\u001b[0m)                    │                             │                 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ time_distributed_13                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m256\u001b[0m)     │               \u001b[38;5;34m0\u001b[0m │\n│ (\u001b[38;5;33mTimeDistributed\u001b[0m)                    │                             │                 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ time_distributed_14                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m256\u001b[0m)             │               \u001b[38;5;34m0\u001b[0m │\n│ (\u001b[38;5;33mTimeDistributed\u001b[0m)                    │                             │                 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ bidirectional (\u001b[38;5;33mBidirectional\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m512\u001b[0m)             │       \u001b[38;5;34m1,050,624\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dropout (\u001b[38;5;33mDropout\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m512\u001b[0m)             │               \u001b[38;5;34m0\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ bidirectional_1 (\u001b[38;5;33mBidirectional\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)                 │         \u001b[38;5;34m656,384\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense (\u001b[38;5;33mDense\u001b[0m)                        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 │          \u001b[38;5;34m32,896\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ batch_normalization_4                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 │             \u001b[38;5;34m512\u001b[0m │\n│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m)                   │             \u001b[38;5;34m258\u001b[0m │\n└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n│ input_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)     │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ time_distributed (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">896</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ time_distributed_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ time_distributed_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">112</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">112</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)    │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ time_distributed_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">112</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">112</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">18,496</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ time_distributed_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">112</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">112</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ time_distributed_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)      │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ time_distributed_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)     │          <span style=\"color: #00af00; text-decoration-color: #00af00\">73,856</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ time_distributed_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)     │         <span style=\"color: #00af00; text-decoration-color: #00af00\">147,584</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ time_distributed_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ time_distributed_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)     │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ time_distributed_10                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)     │         <span style=\"color: #00af00; text-decoration-color: #00af00\">295,168</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>)                    │                             │                 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ time_distributed_11                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)     │         <span style=\"color: #00af00; text-decoration-color: #00af00\">590,080</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>)                    │                             │                 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ time_distributed_12                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)     │           <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>)                    │                             │                 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ time_distributed_13                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)     │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>)                    │                             │                 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ time_distributed_14                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)             │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>)                    │                             │                 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ bidirectional (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)             │       <span style=\"color: #00af00; text-decoration-color: #00af00\">1,050,624</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)             │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ bidirectional_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                 │         <span style=\"color: #00af00; text-decoration-color: #00af00\">656,384</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 │          <span style=\"color: #00af00; text-decoration-color: #00af00\">32,896</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ batch_normalization_4                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 │             <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)                   │             <span style=\"color: #00af00; text-decoration-color: #00af00\">258</span> │\n└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,868,674\u001b[0m (10.94 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,868,674</span> (10.94 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m2,867,458\u001b[0m (10.94 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,867,458</span> (10.94 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m1,216\u001b[0m (4.75 KB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,216</span> (4.75 KB)\n</pre>\n"},"metadata":{}},{"name":"stdout","text":"Epoch 1/10\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:122: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n  self._warn_if_super_not_called()\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - accuracy: 0.4999 - auc: 0.5003 - loss: 1.1235 - precision: 0.4999 - recall: 0.4999\nEpoch 1: val_auc improved from -inf to 0.51429, saving model to best_model.keras\n\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m510s\u001b[0m 4s/step - accuracy: 0.5001 - auc: 0.5005 - loss: 1.1234 - precision: 0.5001 - recall: 0.5001 - val_accuracy: 0.3860 - val_auc: 0.5143 - val_loss: 0.7296 - val_precision: 0.3860 - val_recall: 0.3860 - learning_rate: 1.0000e-04\nEpoch 2/10\n\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - accuracy: 0.5350 - auc: 0.5693 - loss: 1.0132 - precision: 0.5350 - recall: 0.5350\nEpoch 2: val_auc improved from 0.51429 to 0.56691, saving model to best_model.keras\n\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m481s\u001b[0m 4s/step - accuracy: 0.5350 - auc: 0.5693 - loss: 1.0131 - precision: 0.5350 - recall: 0.5350 - val_accuracy: 0.5205 - val_auc: 0.5669 - val_loss: 0.7601 - val_precision: 0.5205 - val_recall: 0.5205 - learning_rate: 1.0000e-04\nEpoch 3/10\n\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - accuracy: 0.5837 - auc: 0.6173 - loss: 0.9067 - precision: 0.5837 - recall: 0.5837\nEpoch 3: val_auc improved from 0.56691 to 0.59771, saving model to best_model.keras\n\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m482s\u001b[0m 4s/step - accuracy: 0.5840 - auc: 0.6177 - loss: 0.9063 - precision: 0.5840 - recall: 0.5840 - val_accuracy: 0.4971 - val_auc: 0.5977 - val_loss: 1.0619 - val_precision: 0.4971 - val_recall: 0.4971 - learning_rate: 1.0000e-04\nEpoch 4/10\n\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - accuracy: 0.6786 - auc: 0.7726 - loss: 0.6292 - precision: 0.6786 - recall: 0.6786\nEpoch 4: val_auc improved from 0.59771 to 0.81439, saving model to best_model.keras\n\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m479s\u001b[0m 4s/step - accuracy: 0.6787 - auc: 0.7727 - loss: 0.6293 - precision: 0.6787 - recall: 0.6787 - val_accuracy: 0.6842 - val_auc: 0.8144 - val_loss: 0.7130 - val_precision: 0.6842 - val_recall: 0.6842 - learning_rate: 1.0000e-04\nEpoch 5/10\n\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - accuracy: 0.7562 - auc: 0.8262 - loss: 0.6041 - precision: 0.7562 - recall: 0.7562\nEpoch 5: val_auc improved from 0.81439 to 0.96224, saving model to best_model.keras\n\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m479s\u001b[0m 4s/step - accuracy: 0.7564 - auc: 0.8265 - loss: 0.6033 - precision: 0.7564 - recall: 0.7564 - val_accuracy: 0.8596 - val_auc: 0.9622 - val_loss: 0.2478 - val_precision: 0.8596 - val_recall: 0.8596 - learning_rate: 1.0000e-04\nEpoch 6/10\n\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - accuracy: 0.7421 - auc: 0.8594 - loss: 0.5099 - precision: 0.7421 - recall: 0.7421\nEpoch 6: val_auc did not improve from 0.96224\n\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m477s\u001b[0m 4s/step - accuracy: 0.7424 - auc: 0.8597 - loss: 0.5093 - precision: 0.7424 - recall: 0.7424 - val_accuracy: 0.5146 - val_auc: 0.6130 - val_loss: 1.6348 - val_precision: 0.5146 - val_recall: 0.5146 - learning_rate: 1.0000e-04\nEpoch 7/10\n\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - accuracy: 0.8506 - auc: 0.9071 - loss: 0.3943 - precision: 0.8506 - recall: 0.8506\nEpoch 7: val_auc improved from 0.96224 to 0.99673, saving model to best_model.keras\n\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m480s\u001b[0m 4s/step - accuracy: 0.8506 - auc: 0.9071 - loss: 0.3942 - precision: 0.8506 - recall: 0.8506 - val_accuracy: 0.9708 - val_auc: 0.9967 - val_loss: 0.0738 - val_precision: 0.9708 - val_recall: 0.9708 - learning_rate: 1.0000e-04\nEpoch 8/10\n\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - accuracy: 0.8389 - auc: 0.9416 - loss: 0.3151 - precision: 0.8389 - recall: 0.8389\nEpoch 8: val_auc improved from 0.99673 to 0.99870, saving model to best_model.keras\n\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m477s\u001b[0m 4s/step - accuracy: 0.8391 - auc: 0.9418 - loss: 0.3145 - precision: 0.8391 - recall: 0.8391 - val_accuracy: 0.9825 - val_auc: 0.9987 - val_loss: 0.0683 - val_precision: 0.9825 - val_recall: 0.9825 - learning_rate: 1.0000e-04\nEpoch 9/10\n\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - accuracy: 0.8734 - auc: 0.9213 - loss: 0.3870 - precision: 0.8734 - recall: 0.8734\nEpoch 9: val_auc improved from 0.99870 to 0.99997, saving model to best_model.keras\n\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m478s\u001b[0m 4s/step - accuracy: 0.8734 - auc: 0.9213 - loss: 0.3868 - precision: 0.8734 - recall: 0.8734 - val_accuracy: 0.9942 - val_auc: 1.0000 - val_loss: 0.0151 - val_precision: 0.9942 - val_recall: 0.9942 - learning_rate: 1.0000e-04\nEpoch 10/10\n\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - accuracy: 0.9162 - auc: 0.9601 - loss: 0.2522 - precision: 0.9162 - recall: 0.9162\nEpoch 10: val_auc did not improve from 0.99997\n\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m478s\u001b[0m 4s/step - accuracy: 0.9160 - auc: 0.9601 - loss: 0.2525 - precision: 0.9160 - recall: 0.9160 - val_accuracy: 0.8538 - val_auc: 0.9080 - val_loss: 0.3758 - val_precision: 0.8538 - val_recall: 0.8538 - learning_rate: 1.0000e-04\nRestoring model weights from the end of the best epoch: 9.\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step\n\nTest Accuracy: 0.9825\n\nClassification Report:\n                precision    recall  f1-score   support\n\nNon-Shoplifter       0.99      0.98      0.99       106\n    Shoplifter       0.97      0.98      0.98        65\n\n      accuracy                           0.98       171\n     macro avg       0.98      0.98      0.98       171\n  weighted avg       0.98      0.98      0.98       171\n\n\nConfusion Matrix:\n[[104   2]\n [  1  64]]\nModel training and evaluation completed!\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}